{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK 6 CHALLENGE - Prediction\n",
    "\n",
    "## Contents\n",
    "* Util.py\n",
    "* Preprocessing.py\n",
    "    * Encoding categorical columns\n",
    "    * Preprocessed data\n",
    "    * Rescaling numerical columns\n",
    "* Data.py\n",
    "    * Dependent and independent variables\n",
    "    * Splitting dataset\n",
    "    * Dimensionality reduction: PCA, TSNE\n",
    "    * Class imbalance: SMOTE\n",
    "* Model.py\n",
    "    * Building model\n",
    "        * Cross validation: KFold, StratifiedKfold\n",
    "        * Evaluation metrics\n",
    "    * Prediction\n",
    "* Main.py\n",
    "    * Dataset\n",
    "    * Preprocessing\n",
    "    * Data\n",
    "    * Machine Learning model\n",
    "        * Logistic Regression\n",
    "        * XGBoost\n",
    "        * Multi Layer Perceptron\n",
    "\n",
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Util.py\n",
    "# import system libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Preprocess.py\n",
    "\n",
    "# import library for preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import libraies for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "import Util\n",
    "\n",
    "# preprocessing class\n",
    "class preprocess:\n",
    "    \n",
    "    # create list containing categorical columns\n",
    "    cat_cols = ['job', 'marital', 'education', 'default', 'housing',\n",
    "                'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "    # create list containing numerical columns\n",
    "    num_cols = ['duration', 'campaign', 'emp.var.rate',\"pdays\",\"age\", 'cons.price.idx', \n",
    "                'cons.conf.idx', 'euribor3m', 'nr.employed', 'previous']\n",
    "    \n",
    "    # function to encode categorical columns\n",
    "    def encode(self, data):\n",
    "        cat_var_enc = pd.get_dummies(data[self.cat_cols], drop_first=False)\n",
    "        return cat_var_enc\n",
    "    \n",
    "    # function to \n",
    "    def preprocessed(self, data):\n",
    "        # adding the encoded columns to the dataframe\n",
    "        data = pd.concat([data, self.encode(data)], axis=1)\n",
    "        # saving the column names of categorical variables\n",
    "        cat_cols_all = list(self.encode(data).columns)\n",
    "        # creating a new dataframe with features and output\n",
    "        cols_input = self.num_cols + cat_cols_all\n",
    "        preprocessed_data = data[cols_input + ['subscribed']]\n",
    "        return preprocessed_data\n",
    "    \n",
    "    # function to rescale numerical columns\n",
    "    def rescale(self, data):\n",
    "        # creating an instance of the scaler object\n",
    "        scaler = StandardScaler()\n",
    "        data[self.num_cols] = scaler.fit_transform(data[self.num_cols])\n",
    "        return data\n",
    "    \n",
    "# create class methods\n",
    "preprocess.encode = classmethod(preprocess.encode)\n",
    "encode = preprocess.encode\n",
    "preprocess.preprocessed = classmethod(preprocess.preprocessed)\n",
    "preprocessed = preprocess.preprocessed\n",
    "preprocess.rescale = classmethod(preprocess.rescale)\n",
    "rescale = preprocess.rescale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Data.py\n",
    "\n",
    "# importing all necessary libraries\n",
    "\n",
    "# import methods from Preprocess.py\n",
    "from Preprocess import encode, preprocessed, rescale\n",
    "\n",
    "# import libraies for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# import libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import library for splitting dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import library for dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# import library for dealing with class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# function to get the dependent and independent variable\n",
    "def data_loader(data):\n",
    "    X = data.drop(columns=[ \"subscribed\", 'duration'])\n",
    "    y = data[\"subscribed\"]\n",
    "    print(\"X shape:\",X.shape)\n",
    "    print(\"y shape:\",y.shape)\n",
    "    return X,y\n",
    "\n",
    "# function to split dataset\n",
    "def split_data(X, y):\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1,random_state=1)\n",
    "    # printing the shape of training set\n",
    "    print(f'Train set X shape: {X_train.shape}')\n",
    "    print(f'Train set y shape: {y_train.shape}')\n",
    "    # printing the shape of test set\n",
    "    print(f'Test set X shape: {X_test.shape}')\n",
    "    print(f'Test set y shape: {y_test.shape}')\n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "# function to get the number of components for dimensionality reduction\n",
    "def pca(data):\n",
    "    # create an instance of pca\n",
    "    pca = PCA()\n",
    "    # fit pca to our data\n",
    "    pca.fit(data)\n",
    "    # saving the explained variance ratio\n",
    "    explained = pca.explained_variance_ratio_\n",
    "    # plot the cumulative variance explained by total number of components\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(range(1,61), explained.cumsum(), marker='o', linestyle='--')\n",
    "    plt.title('Explained Variance by Components')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cummulative Explained Variance')\n",
    "    plt.savefig('pca.png')\n",
    "    plt.show()\n",
    "\n",
    "# function to reduce dimensions\n",
    "def dimension_reduction(method, components, train_data, test_data):\n",
    "    # PCA\n",
    "    if (method == 'PCA'):\n",
    "        pca = PCA(n_components=components)\n",
    "        pca.fit(train_data)\n",
    "        pca_train = pca.transform(train_data)\n",
    "        X_train_reduced = pd.DataFrame(pca_train)\n",
    "        print(\"original shape:   \", train_data.shape)\n",
    "        print(\"transformed shape:\", X_train_reduced.shape)\n",
    "        print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "        # applying method transform to X_test\n",
    "        pca_test = pca.transform(test_data)\n",
    "        X_test_reduced = pd.DataFrame(pca_test)\n",
    "        \n",
    "    # TSNE\n",
    "    elif (method == 'TSNE'):\n",
    "        tsne = TSNE(n_components=components)\n",
    "        tsne_train = tsne.fit_transform(train_data)\n",
    "        X_train_reduced = pd.DataFrame(tsne_train)\n",
    "        print(\"original shape:   \", train_data.shape)\n",
    "        print(\"transformed shape:\", X_train_reduced.shape)\n",
    "        # applying method transform to X_test\n",
    "        tsne_test = tsne.fit_transform(test_data)\n",
    "        X_test_reduced = pd.DataFrame(tsne_test)\n",
    "    \n",
    "    else:\n",
    "        print('Dimensionality reduction method not found!')\n",
    "        \n",
    "    return X_train_reduced, X_test_reduced\n",
    "\n",
    "# function to deal with imbalanced class\n",
    "def class_imbalance(X_data, y_data):\n",
    "    # creating an instance\n",
    "    sm = SMOTE(random_state=27)\n",
    "    # applying it to the data\n",
    "    X_train_smote, y_train_smote = sm.fit_sample(X_data, y_data)\n",
    "    return X_train_smote, y_train_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Model.py\n",
    "\n",
    "# import all necessary libraries\n",
    "\n",
    "# import methods from data.py\n",
    "from Data import data_loader, split_data, pca\n",
    "from Data import dimension_reduction, class_imbalance\n",
    "\n",
    "# import libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import machine learning model libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# import libraries for cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# import evaluation metrics\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_recall_curve, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# function to build machine learning models\n",
    "def model(model, cv_method, metrics, X_train, X_test, y_train):\n",
    "    if (model == 'LR'):\n",
    "        # creating an instance of the regression\n",
    "        model_inst = LogisticRegression()\n",
    "        print('Logistic Regression\\n----------------------')\n",
    "    elif (model == 'XGB'):\n",
    "        # creating an instance of the classifier\n",
    "        model_inst = XGBClassifier()\n",
    "        print('XGBoost\\n----------------------')\n",
    "    elif (model == 'MLP'):\n",
    "        # creating an instance of the classifier\n",
    "        model_inst = MLPClassifier()\n",
    "        print('Multi Layer Perceptron\\n----------------------')\n",
    "    elif (model == 'SVM'):\n",
    "        # creating an instance of the classifier\n",
    "        kernel = input('Enter the kernel (rbf, linear, or poly):')\n",
    "        model_inst = SVC(kernel=kernel, C=1.0)\n",
    "        print('Support Vector Classification\\n----------------------')\n",
    "    \n",
    "    # cross validation\n",
    "    if (cv_method == 'KFold'):\n",
    "        print('Cross validation: KFold\\n--------------------------')\n",
    "        cv = KFold(n_splits=10, random_state=100)\n",
    "    elif (cv_method == 'StratifiedKFold'):\n",
    "        print('Cross validation: StratifiedKFold\\n--------------------------')\n",
    "        cv = StratifiedKFold(n_splits=10, random_state=100)\n",
    "    else:\n",
    "        print('Cross validation method not found!')\n",
    "    try:\n",
    "        cv_scores = cross_validate(model_inst, X_train, y_train, \n",
    "                                   cv=cv, scoring=metrics)   \n",
    "        # displaying evaluation metric scores\n",
    "        cv_metric = cv_scores.keys()\n",
    "        for metric in cv_metric:\n",
    "            mean_score = cv_scores[metric].mean()*100\n",
    "            print(metric+':', '%.2f%%' % mean_score)\n",
    "            print('')\n",
    "            \n",
    "    except:\n",
    "        metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "        cv_scores = cross_validate(model_inst, X_train, y_train, \n",
    "                                   cv=cv, scoring=metrics)\n",
    "        # displaying evaluation metric scores\n",
    "        cv_metric = cv_scores.keys()\n",
    "        for metric in cv_metric:\n",
    "            mean_score = cv_scores[metric].mean()*100\n",
    "            print(metric+':', '%.2f%%' % mean_score)\n",
    "            print('')\n",
    "\n",
    "    return model_inst\n",
    "    \n",
    "# function to make predictions\n",
    "def prediction(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    model_ = model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    #Get the confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cf_matrix, annot=True, fmt='.0f')\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.savefig(f'conf_{model_name}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main - Dataset without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Main.py\n",
    "\n",
    "# import all necessary libraries\n",
    "\n",
    "# import library to get working directory\n",
    "import os\n",
    "\n",
    "# import libraies for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import Util\n",
    "\n",
    "# import methods from Preprocess.py\n",
    "from Preprocess import encode, preprocessed, rescale\n",
    "\n",
    "# import methods from Data.py\n",
    "from Data import data_loader, split_data, pca\n",
    "from Data import dimension_reduction, class_imbalance\n",
    "\n",
    "# import methods from Model.py\n",
    "from Model import model, prediction\n",
    "\n",
    "# Dataset\n",
    "# function to change working directory\n",
    "def change_dir(path):\n",
    "    print(\"Old directory: \",os.getcwd())\n",
    "    os.chdir(path)\n",
    "    print(\"New directory: \",os.getcwd())\n",
    "    \n",
    "# changing the working directory to access the dataset\n",
    "change_dir('C:\\\\Users\\\\PC\\\\Desktop\\\\Data Science\\\\10 Academy\\\\Training\\\\Week 6\\\\Challenge\\\\Dataset')    \n",
    "\n",
    "# import the dataset without outliers\n",
    "dataset_new = pd.read_csv('bank-addition-full-without-outliers.csv')\n",
    "dataset_new.name = 'New dataset'\n",
    "print(\"New Dataset\\n-------------------------\")\n",
    "print(dataset_new.head())\n",
    "\n",
    "# changing the working directory to back to original working directory\n",
    "change_dir('C:\\\\Users\\\\PC\\\\Desktop\\\\Data Science\\\\10 Academy\\\\Training\\\\Week 6\\\\Challenge\\\\Notebooks')\n",
    "\n",
    "# Preprocessing - Using the new dataset i.e. data without outliers\n",
    "# replacing basic.4y, basic.6y, basic.9y as basic\n",
    "dataset_new['education'] = dataset_new['education'].replace(['basic.4y', 'basic.6y', 'basic.9y'], 'basic')\n",
    "\n",
    "# defining output variable for classification\n",
    "dataset_new['subscribed'] = (dataset_new.y == 'yes').astype('int')\n",
    "\n",
    "# encoding categorical columns\n",
    "encoded_data = encode(dataset_new)\n",
    "print(\"Encoded Data\\n-------------------------\")\n",
    "print(encoded_data.head())\n",
    "\n",
    "# preprocessed data\n",
    "preprocessed_data = preprocessed(dataset_new)\n",
    "print(\"Preprocessed Data\\n-------------------------\")\n",
    "print(preprocessed_data.head())\n",
    "\n",
    "# rescaling numerical columns\n",
    "preprocessed_data = rescale(preprocessed_data)\n",
    "print(\"Rescaled Data\\n-------------------------\")\n",
    "print(preprocessed_data.head())\n",
    "\n",
    "# Data\n",
    "# dependent and independent variables\n",
    "X, y = data_loader(preprocessed_data)\n",
    "\n",
    "# splitting the data\n",
    "X_train,X_test,y_train,y_test = split_data(X, y)\n",
    "\n",
    "# pca visualization to get number of components\n",
    "pca(X_train)\n",
    "\n",
    "# dimensionality reduction\n",
    "X_train_reduced, X_test_reduced = dimension_reduction('PCA', 20, X_train, X_test)\n",
    "\n",
    "# dealing with imbalanced class\n",
    "X_train_smote, y_train_smote = class_imbalance(X_train_reduced, y_train)\n",
    "\n",
    "# machine learning model\n",
    "metrics = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']\n",
    "\n",
    "# 1. Logistic Regression\n",
    "# KFold cross validation\n",
    "model_res = model('LR', 'KFold', metrics, X_train_smote, X_test_reduced, y_train_smote)\n",
    "# StratifiedKFold cross validation\n",
    "model_res = model('LR', 'StratifiedKFold', metrics, X_train_smote, X_test_reduced, y_train_smote)\n",
    "# make prediction\n",
    "prediction(model_res, 'Linear Regression', X_train_smote, y_train_smote, X_test_reduced, y_test)\n",
    "\n",
    "# 2. XGBoost\n",
    "# KFold cross validation\n",
    "model_res = model('XGB', 'KFold', metrics, X_train_smote, X_test_reduced, y_train_smote)\n",
    "# StratifiedKFold cross validation\n",
    "model_res = model('XGB', 'StratifiedKFold', metrics, X_train_smote, X_test_reduced, y_train_smote)\n",
    "# make prediction\n",
    "prediction(model_res, 'XGBoost Classifier', X_train_smote, y_train_smote, X_test_reduced, y_test)\n",
    "\n",
    "# 3. Multi Layer Perceptron\n",
    "# KFold cross validation\n",
    "model_res = model('MLP', 'KFold', metrics, X_train_smote, X_test_reduced, y_train_smote)\n",
    "# StratifiedKFold cross validation\n",
    "model_res = model('MLP', 'StratifiedKFold', metrics, X_train_smote, X_test_reduced, y_train_smote)\n",
    "# make prediction\n",
    "prediction(model_res, 'Multi Layer Perceptron', X_train_smote, y_train_smote, X_test_reduced, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main - Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Main_org.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Main_org.py\n",
    "\n",
    "# import all necessary libraries\n",
    "\n",
    "# import library to get working directory\n",
    "import os\n",
    "\n",
    "# import libraies for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import Util\n",
    "\n",
    "# import methods from Preprocess.py\n",
    "from Preprocess import encode, preprocessed, rescale\n",
    "\n",
    "# import methods from Data.py\n",
    "from Data import data_loader, split_data, pca\n",
    "from Data import dimension_reduction, class_imbalance\n",
    "\n",
    "# import methods from Model.py\n",
    "from Model import model, prediction\n",
    "\n",
    "# Dataset\n",
    "# function to change working directory\n",
    "def change_dir(path):\n",
    "    print(\"Old directory: \",os.getcwd())\n",
    "    os.chdir(path)\n",
    "    print(\"New directory: \",os.getcwd())\n",
    "    \n",
    "# changing the working directory to access the dataset\n",
    "change_dir('C:\\\\Users\\\\PC\\\\Desktop\\\\Data Science\\\\10 Academy\\\\Training\\\\Week 6\\\\Challenge\\\\Dataset')    \n",
    "\n",
    "# import the original dataset\n",
    "dataset = pd.read_csv('bank-additional-full.csv', sep=';')\n",
    "dataset.name = 'dataset'\n",
    "print(\"Original Dataset\\n-------------------------\")\n",
    "print(dataset.head())\n",
    "\n",
    "# changing the working directory to back to original working directory\n",
    "change_dir('C:\\\\Users\\\\PC\\\\Desktop\\\\Data Science\\\\10 Academy\\\\Training\\\\Week 6\\\\Challenge\\\\Notebooks')\n",
    "\n",
    "# Preprocessing - Using the new dataset i.e. data without outliers\n",
    "# replacing basic.4y, basic.6y, basic.9y as basic\n",
    "dataset['education'] = dataset['education'].replace(['basic.4y', 'basic.6y', 'basic.9y'], 'basic')\n",
    "\n",
    "# defining output variable for classification\n",
    "dataset['subscribed'] = (dataset.y == 'yes').astype('int')\n",
    "\n",
    "# encoding categorical columns\n",
    "encoded_data = encode(dataset)\n",
    "print(\"Encoded Data\\n-------------------------\")\n",
    "print(encoded_data.head())\n",
    "\n",
    "# preprocessed data\n",
    "preprocessed_data = preprocessed(dataset)\n",
    "print(\"Preprocessed Data\\n-------------------------\")\n",
    "print(preprocessed_data.head())\n",
    "\n",
    "# rescaling numerical columns\n",
    "preprocessed_data = rescale(preprocessed_data)\n",
    "print(\"Rescaled Data\\n-------------------------\")\n",
    "print(preprocessed_data.head())\n",
    "\n",
    "# Data\n",
    "# dependent and independent variables\n",
    "X, y = data_loader(preprocessed_data)\n",
    "\n",
    "# splitting the data\n",
    "X_train,X_test,y_train,y_test = split_data(X, y)\n",
    "\n",
    "# pca visualization to get number of components\n",
    "pca(X_train)\n",
    "\n",
    "# dimensionality reduction\n",
    "X_train_reduced, X_test_reduced = dimension_reduction('PCA', 20, X_train, X_test)\n",
    "\n",
    "# dealing with imbalanced class\n",
    "X_train_smote, y_train_smote = class_imbalance(X_train_reduced, y_train)\n",
    "\n",
    "# machine learning model\n",
    "metrics = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']\n",
    "\n",
    "# 1. Logistic Regression\n",
    "# KFold cross validation\n",
    "model_res = model('LR', 'KFold', metrics, X_train_smote, X_test_reduced, y_train_smote)\n",
    "# StratifiedKFold cross validation\n",
    "model_res = model('LR', 'StratifiedKFold', metrics, X_train_smote, X_test_reduced, y_train_smote)\n",
    "# make prediction\n",
    "prediction(model_res, 'Linear Regression', X_train_smote, y_train_smote, X_test_reduced, y_test)\n",
    "\n",
    "# 2. XGBoost\n",
    "# KFold cross validation\n",
    "model_res = model('XGB', 'KFold', metrics, X_train_smote, X_test_reduced, y_train_smote)\n",
    "# StratifiedKFold cross validation\n",
    "model_res = model('XGB', 'StratifiedKFold', metrics, X_train_smote, X_test_reduced, y_train_smote)\n",
    "# make prediction\n",
    "prediction(model_res, 'XGBoost Classifier', X_train_smote, y_train_smote, X_test_reduced, y_test)\n",
    "\n",
    "# 3. Multi Layer Perceptron\n",
    "# KFold cross validation\n",
    "model_res = model('MLP', 'KFold', metrics, X_train_smote, X_test_reduced, y_train_smote)\n",
    "# StratifiedKFold cross validation\n",
    "model_res = model('MLP', 'StratifiedKFold', metrics, X_train_smote, X_test_reduced, y_train_smote)\n",
    "# make prediction\n",
    "prediction(model_res, 'Multi Layer Perceptron', X_train_smote, y_train_smote, X_test_reduced, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
